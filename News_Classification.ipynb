{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News_Classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1U9-R5xyTQs3iTMhS_P-XqMLdmzin37mP",
      "authorship_tag": "ABX9TyMfxhNMDHRRJgK+kSMp4tZi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prabha-yadav/Vehicle-Price-Prediction/blob/main/News_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vLxMa76FOFj"
      },
      "source": [
        "Problem Statement:\n",
        "Many news aggregators face the challenge of common news stories published from many\n",
        "providers. The importance of grouping based on similarity is very much important, so it can save\n",
        "end usersâ€™ reading time. This is a more sort of clustering problem where similar articles should\n",
        "be in the same group."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q_KAtJBFVV_"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96ehk52cGewm"
      },
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyHwtsi9QRZf"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Data_science/uci-news-aggregator.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aggvg_iWQZoP"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNpF4JSy71gK"
      },
      "source": [
        "df['CATEGORY'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgsnmfMk6tXE"
      },
      "source": [
        "# Converting values in 'CATEGORY' column to numerica values and create a new column as 'Category'.\n",
        "\n",
        "df['Category']=df['CATEGORY'].map({'b':0, 't':1, 'e':2, 'm':3})\n",
        "df['Category'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1pEOUDZ70oo"
      },
      "source": [
        "# As mentioned in problem statement, we need to analyse column 'TITLE' and 'Category', to carryout the analysis. So, selecting these columns only in \n",
        "# 'Data'. \n",
        "\n",
        "data = df[['TITLE', 'Category']]\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xngdjGwhCyZM"
      },
      "source": [
        "# remove na values\n",
        "\n",
        "data = data.dropna()\n",
        "data.head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aibGoIKVE-OL"
      },
      "source": [
        "# 'reset index' in case some data is deleted while dropping null values in the above step.\n",
        "\n",
        "dataset = data.copy() \n",
        "dataset.reset_index(inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHP6XeuEHsGG"
      },
      "source": [
        "dataset.head(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aRqaD32H5HA"
      },
      "source": [
        "# get the independent feature\n",
        "\n",
        "X = dataset['TITLE']\n",
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z1TjSqkMT14"
      },
      "source": [
        "# get dependent feature\n",
        "y = dataset['Category']\n",
        "y.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOtZhoqjMkQp"
      },
      "source": [
        "# Lets find the unique feature of depenedent value\n",
        "\n",
        "y.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AZu4UK4OvUd"
      },
      "source": [
        "X.shape\n",
        "y.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-Vzoi9H5Dyp"
      },
      "source": [
        "# cleaning the data\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "corpus = []\n",
        "for i in range(0, len(dataset)):\n",
        "  # remove all the characters and text except A-Z and a-z in 'TITLE' coulumn\n",
        "    review = re.sub('[^a-zA-Z]', ' ', dataset['TITLE'][i])\n",
        "  # lower the font case   \n",
        "    review = review.lower()\n",
        "  # split all the characters so that stopwords can be performed\n",
        "    review = review.split()\n",
        "    \n",
        "  #  Implementing 'Bag Of Words'\n",
        "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y32csCIzKbTx"
      },
      "source": [
        "corpus[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0haUazoK7qU"
      },
      "source": [
        "## Applying Countvectorizer\n",
        "# Creating the Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features=5000,ngram_range=(1,3))\n",
        "X = cv.fit_transform(corpus).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6VLOx75PKk7"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRSmTAbNPOCj"
      },
      "source": [
        "## Divide the dataset into Train and Test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FctLsZgCPljZ"
      },
      "source": [
        "cv.get_feature_names()[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34c4VGD6Po9y"
      },
      "source": [
        "cv.get_params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VOtnlc2PpvF"
      },
      "source": [
        "count_df = pd.DataFrame(X_train, columns=cv.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGmys8-gP7oH"
      },
      "source": [
        "count_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LtjYaiTP_m-"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXSe_ksPQEeS"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    See full source and example: \n",
        "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "    \n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.TITLE(TITLE)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "    # print precision, recall and f1 matrics\n",
        "    print(metrics.classification_report(y_true, y_pred, digits =3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wgTqu1NQwj9"
      },
      "source": [
        "Applying MultuNomialNB Algo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dIXLgbxRSWh"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier=MultinomialNB()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tJbZ_fzRbS8"
      },
      "source": [
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyem2HyGRc1n"
      },
      "source": [
        "classifier.fit(X_train, y_train)\n",
        "pred = classifier.predict(X_test)\n",
        "score = metrics.accuracy_score(y_test, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp9oVrgyQWvu"
      },
      "source": [
        "print(\"accuracy:   %0.3f\" % score)\n",
        "cm = metrics.confusion_matrix(y_test, pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}