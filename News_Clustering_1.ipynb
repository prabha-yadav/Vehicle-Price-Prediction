{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News_Classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1U9-R5xyTQs3iTMhS_P-XqMLdmzin37mP",
      "authorship_tag": "ABX9TyMV0fdu2cjL5lqdFDyfnrfP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prabha-yadav/Vehicle-Price-Prediction/blob/main/News_Clustering_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vLxMa76FOFj"
      },
      "source": [
        "Problem Statement:\n",
        "Many news aggregators face the challenge of common news stories published from many\n",
        "providers. The importance of grouping based on similarity is very much important, so it can save\n",
        "end usersâ€™ reading time. This is a more sort of clustering problem where similar articles should\n",
        "be in the same group."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q_KAtJBFVV_"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96ehk52cGewm"
      },
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyHwtsi9QRZf"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Data_science/uci-news-aggregator.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aggvg_iWQZoP"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjn_SliQjzed"
      },
      "source": [
        "data = df['STORY']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh5kGwOmkohK"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xngdjGwhCyZM"
      },
      "source": [
        "# remove na values\n",
        "\n",
        "dataset = data.dropna()\n",
        "dataset.head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aRqaD32H5HA"
      },
      "source": [
        "# get the independent feature\n",
        "\n",
        "X = df['STORY']\n",
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AZu4UK4OvUd"
      },
      "source": [
        "X.shape\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-Vzoi9H5Dyp"
      },
      "source": [
        "# cleaning the data\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "corpus = []\n",
        "for i in range(0, len(dataset)):\n",
        "  # remove all the characters and text except A-Z and a-z in 'TITLE' coulumn\n",
        "    review = re.sub('[^a-zA-Z]', ' ', dataset['TITLE'][i])\n",
        "  # lower the font case   \n",
        "    review = review.lower()\n",
        "  # split all the characters so that stopwords can be performed\n",
        "    review = review.split()\n",
        "    \n",
        "  #  Implementing 'Bag Of Words'\n",
        "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y32csCIzKbTx"
      },
      "source": [
        "corpus[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0haUazoK7qU"
      },
      "source": [
        "## Applying Countvectorizer\n",
        "# Creating the Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features=5000,ngram_range=(1,3))\n",
        "X = cv.fit_transform(corpus).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6VLOx75PKk7"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LtjYaiTP_m-"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXSe_ksPQEeS"
      },
      "source": [
        "#Using the elbow method to find the optimal number of clusters or K value (value at X-axis).\n",
        "\n",
        "wcss=[]\n",
        "for i in range(1,11):\n",
        "    kmeans=KMeans(n_clusters=i, init='k-means++',random_state=0)\n",
        "    kmeans.fit(X)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(range(1,11),wcss)\n",
        "plt.title('The Elbow Method')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()\n",
        "\n",
        "# since the code could not be executed, as so K value could not be executed, so I assume K value as 5, to visualize the cluster."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpsVinMIs_WK"
      },
      "source": [
        "#Fitting K-MEans to the dataset, for different vlaues of n_clusters = 1,2,3,4,5\n",
        "kmeans=KMeans(n_clusters=5,init='k-means++',random_state=0)\n",
        "y_kmeans=kmeans.fit_predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiBlaqXQtEwu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}